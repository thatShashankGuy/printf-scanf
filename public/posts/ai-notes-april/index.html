<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Brief Notes on GenAI from April | PrintF/ScanF</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="1 · Machine–Learning Model Taxonomy 🔗1.1 Discriminative vs Generative 🔗 Discriminative models learn the conditional distribution $P(y\mid x)$ to separate classes.
Primary use · classification/regression. Typical nature · often deterministic at inference. Examples · Logistic Regression, SVM, plain Feed‑forward NNs, ResNet‑like CNNs. Generative models learn the joint distribution $P(x,y)=P(x\mid y)P(y)$ and can synthesize new x.
Primary use · both classification and data generation/simulation. Typical nature · probabilistic/stochastic. Examples · Naïve Bayes, Hidden Markov Models, GANs, VAEs, Diffusion models.">
<meta name="generator" content="Hugo 0.126.1">


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">



<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />








  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">←</span>Home</a>
	
	<a href="/about">About</a>
	<a href="/externals">References</a>
	<a href="/posts">Archive</a>
	<a href="/tags" style="display: none;">Tags</a>	
	

	

	
	
	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Brief Notes on GenAI from April</h1>

    <div class="tip">
        <time datetime="2025-05-02 00:00:00 &#43;0000 UTC">May 2, 2025</time>
        <span class="split">
          ·
        </span>
        <span>
          811 words
        </span>
        <span class="split">
          ·
        </span>
        <span>
          4 minute read
        </span>
    </div>

    
    


    <div class="content">
      <p><p class="markdown-image">
  <img src="/notesonai.png" alt="headline image"  title="Note taking with AI" />
</p></p>
<h2 id="1-machinelearning-model-taxonomy">1 · Machine–Learning Model Taxonomy <a href="#1-machinelearning-model-taxonomy" class="anchor">🔗</a></h2><h3 id="11discriminativevsgenerative">1.1 Discriminative vs Generative <a href="#11discriminativevsgenerative" class="anchor">🔗</a></h3><ul>
<li>
<p><strong>Discriminative models</strong> learn the conditional distribution $P(y\mid x)$ to <strong>separate</strong> classes.</p>
<ul>
<li><em>Primary use</em> · classification/regression.</li>
<li><em>Typical nature</em> · often deterministic at inference.</li>
<li><em>Examples</em> · Logistic Regression, SVM, plain Feed‑forward NNs, ResNet‑like CNNs.</li>
</ul>
</li>
<li>
<p><strong>Generative models</strong> learn the joint distribution $P(x,y)=P(x\mid y)P(y)$ and can <strong>synthesize new x</strong>.</p>
<ul>
<li><em>Primary use</em> · both classification <strong>and</strong> data generation/simulation.</li>
<li><em>Typical nature</em> · probabilistic/stochastic.</li>
<li><em>Examples</em> · Naïve Bayes, Hidden Markov Models, GANs, VAEs, Diffusion models.</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Discriminative</th>
<th>Generative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learns</td>
<td>$P(y\mid x)$</td>
<td>$P(x,y)$ or $P(x\mid y)P(y)$</td>
</tr>
<tr>
<td>Produces</td>
<td>Class label / score</td>
<td>Sample, likelihood <strong>or</strong> label</td>
</tr>
<tr>
<td>Strength</td>
<td>Decision boundaries</td>
<td>Full data distribution</td>
</tr>
<tr>
<td>Weakness</td>
<td>Needs labelled data</td>
<td>Harder to train/scale</td>
</tr>
</tbody>
</table>
<h3 id="12deterministicvsprobabilistic">1.2 Deterministic vs Probabilistic <a href="#12deterministicvsprobabilistic" class="anchor">🔗</a></h3><table>
<thead>
<tr>
<th>Aspect</th>
<th>Deterministic</th>
<th>Probabilistic</th>
</tr>
</thead>
<tbody>
<tr>
<td>Output</td>
<td>Same value for same input</td>
<td>Distribution or random sample</td>
</tr>
<tr>
<td>Pros</td>
<td>Fast, predictable</td>
<td>Express uncertainty, robust to missing data</td>
</tr>
<tr>
<td>Cons</td>
<td>Ignores uncertainty</td>
<td>More compute, stochastic results</td>
</tr>
<tr>
<td>Examples</td>
<td>Decision‑Tree inference, SVM, frozen CNN</td>
<td>Naïve Bayes, Gaussian Mixture, Bayesian Network, VAE decoder</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="2-vectors-modalities">2 · Vectors &amp; Modalities <a href="#2-vectors-modalities" class="anchor">🔗</a></h2><h3 id="21vectorsembeddings">2.1 Vectors (Embeddings) <a href="#21vectorsembeddings" class="anchor">🔗</a></h3><p><em>Definition</em> · Numeric representations that encode the semantics of entities (words, images, users …).</p>
<ul>
<li>Enable similarity search (<code>cosine</code>, <code>dot</code>), clustering, recommendation, retrieval‑augmented generation.</li>
<li>Learned via Word2Vec, GloVe, fastText, BERT, CLIP, etc.</li>
</ul>
<h3 id="22modalvsmultimodal">2.2 Modal vs Multimodal <a href="#22modalvsmultimodal" class="anchor">🔗</a></h3><ul>
<li>
<p><strong>Modal</strong> = one input type (text <em>or</em> image <em>or</em> audio).</p>
</li>
<li>
<p><strong>Multimodal</strong> = multiple types jointly (e.g., image + caption).</p>
<ul>
<li>Rising trend: vision–language models (e.g., GPT‑4V, Gemini), audio‑text (e.g., Whisper), video‑text.</li>
</ul>
</li>
</ul>
<h3 id="23neural-languagemodels">2.3 Neural Language Models <a href="#23neural-languagemodels" class="anchor">🔗</a></h3><p>Large neural nets pretrained on text with <strong>self‑supervised</strong> objectives.</p>
<ul>
<li>Families: <strong>GPT‑n</strong> (decoder‑only), <strong>BERT</strong> (encoder‑only), <strong>T5 / BART</strong> (encoder‑decoder), <strong>LLaMA</strong> series.</li>
<li>Tasks: text generation, summarization, translation, code completion, search ranking.</li>
</ul>
<hr>
<h2 id="3-transformer-family">3 · Transformer Family <a href="#3-transformer-family" class="anchor">🔗</a></h2><h3 id="31core-architecture-attentionisallyouneed2017">3.1 Core Architecture  (“Attention is All You Need”, 2017) <a href="#31core-architecture-attentionisallyouneed2017" class="anchor">🔗</a></h3><ol>
<li>
<p><strong>Input embeddings + positional encodings</strong>.</p>
</li>
<li>
<p>Repeated <em>N</em> × blocks:</p>
<ul>
<li>Multi‑head <em>self‑attention</em>  → Add &amp; LayerNorm.</li>
<li>Position‑wise Feed‑Forward NN  → Add &amp; LayerNorm.</li>
</ul>
</li>
<li>
<p>(Decoder adds masked self‑attention + cross‑attention to encoder outputs.)</p>
</li>
</ol>
<h3 id="32selfattentionquickintuition">3.2 Self‑Attention (Quick Intuition) <a href="#32selfattentionquickintuition" class="anchor">🔗</a></h3><p>Each token forms a <strong>Query (Q)</strong> vector that is matched against <strong>Keys (K)</strong> of every token; the similarities weight the <strong>Values (V)</strong> to build a context‑aware representation.</p>
<h3 id="33transformervariants">3.3 Transformer Variants <a href="#33transformervariants" class="anchor">🔗</a></h3><table>
<thead>
<tr>
<th>Variant</th>
<th>Architecture</th>
<th>Flagship Models</th>
<th>Typical Tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoder‑only</strong></td>
<td>Auto‑encoding</td>
<td>BERT, RoBERTa</td>
<td>Classification, QA, embeddings</td>
</tr>
<tr>
<td><strong>Decoder‑only</strong></td>
<td>Auto‑regressive</td>
<td>GPT, LLaMA‑2 Chat</td>
<td>Text/code generation</td>
</tr>
<tr>
<td><strong>Encoder‑Decoder</strong></td>
<td>Seq‑to‑Seq</td>
<td>T5, BART, Pegasus</td>
<td>Translation, summarization</td>
</tr>
</tbody>
</table>
<h3 id="34languagemodeling-objectives">3.4 Language‑Modeling Objectives <a href="#34languagemodeling-objectives" class="anchor">🔗</a></h3><table>
<thead>
<tr>
<th>Objective</th>
<th>Context Used</th>
<th>Predicts</th>
<th>Archetype</th>
</tr>
</thead>
<tbody>
<tr>
<td>Masked (MLM)</td>
<td>Bidirectional</td>
<td>Masked tokens</td>
<td>BERT</td>
</tr>
<tr>
<td>Autoregressive (AR)</td>
<td>Left‑to‑right (or right‑to‑left)</td>
<td>Next token</td>
<td>GPT</td>
</tr>
<tr>
<td>Prefix LM</td>
<td>Past tokens (unmasked) + full prefix</td>
<td>Next token</td>
<td>T5 pre‑training</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="4-learning-paradigms">4 · Learning Paradigms <a href="#4-learning-paradigms" class="anchor">🔗</a></h2><table>
<thead>
<tr>
<th>Paradigm</th>
<th>Labels</th>
<th>Model Learns</th>
<th>Canonical Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Supervised</strong></td>
<td>Explicit</td>
<td>Map <em>x → y</em></td>
<td>ImageNet classification</td>
</tr>
<tr>
<td><strong>Unsupervised</strong></td>
<td>None</td>
<td>Structure of <em>x</em></td>
<td>Word2Vec, PCA, clustering</td>
</tr>
<tr>
<td><strong>Self‑Supervised</strong></td>
<td>Labels from data</td>
<td>Predict masked / future parts</td>
<td>GPT pre‑training, BERT MLM</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="5-model-adaptation-finetuning">5 · Model Adaptation &amp; Fine‑Tuning <a href="#5-model-adaptation-finetuning" class="anchor">🔗</a></h2><table>
<thead>
<tr>
<th>Technique</th>
<th>Data Need</th>
<th>Compute</th>
<th>Brief</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prompt Engineering</td>
<td>None (zero‑shot)</td>
<td>–</td>
<td>Steer behavior via instructions/examples</td>
</tr>
<tr>
<td>Supervised Fine‑Tuning (SFT)</td>
<td>Labelled pairs</td>
<td>High</td>
<td>Adjust all weights to task domain</td>
</tr>
<tr>
<td><strong>LoRA / Adapters</strong></td>
<td>Labelled pairs</td>
<td>Low–Med</td>
<td>Train tiny rank‑update layers; mergeable</td>
</tr>
<tr>
<td>RLHF</td>
<td>Human preference scores</td>
<td>Very High</td>
<td>Align model to helpful/safe outputs</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="6-prompt-engineering">6 · Prompt Engineering <a href="#6-prompt-engineering" class="anchor">🔗</a></h2><h3 id="61anatomy-of-a-good-prompt">6.1 Anatomy of a Good Prompt <a href="#61anatomy-of-a-good-prompt" class="anchor">🔗</a></h3><p><strong>Instruction → Context → Input → Output‑format → Tone/Role</strong></p>
<h3 id="62prompting-techniques">6.2 Prompting Techniques <a href="#62prompting-techniques" class="anchor">🔗</a></h3><table>
<thead>
<tr>
<th>Technique</th>
<th>Best For</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zero‑Shot</td>
<td>Simple, common tasks</td>
<td>Ask directly</td>
</tr>
<tr>
<td>Few‑Shot</td>
<td>Pattern imitation</td>
<td>Give 2–5 exemplars</td>
</tr>
<tr>
<td>Chain‑of‑Thought</td>
<td>Reasoning/maths</td>
<td>“Let’s think step by step”</td>
</tr>
<tr>
<td>Self‑Consistency</td>
<td>Reliable CoT answer</td>
<td>Sample K reasoning paths, majority vote</td>
</tr>
<tr>
<td>ReAct</td>
<td>Tool‑using agents</td>
<td>Interleave reasoning &amp; external actions</td>
</tr>
<tr>
<td>Tree‑of‑Thought</td>
<td>Complex planning</td>
<td>Explore multiple branches, backtrack</td>
</tr>
<tr>
<td>Retrieval‑Augmented (RAG)</td>
<td>Factual or domain answers</td>
<td>Retrieve docs → feed as context</td>
</tr>
</tbody>
</table>
<h3 id="63security-concerns-in-prompting">6.3 Security Concerns in Prompting <a href="#63security-concerns-in-prompting" class="anchor">🔗</a></h3><ol>
<li><strong>Prompt injection / jailbreaks</strong></li>
<li><strong>Data leakage</strong> (keys, PII)</li>
<li><strong>Prompt leakage</strong> (system prompt exposure)</li>
<li><strong>Malicious content generation</strong> (spam, phishing, code exploits)</li>
<li><strong>Token flooding / prompt DOS</strong></li>
</ol>
<p><em>Mitigations</em> · input sanitation, guardrail LLMs, content filters, max‑token limits, rate‑limits, red‑teaming.</p>
<hr>
<h2 id="7-foundation-models">7 · Foundation Models <a href="#7-foundation-models" class="anchor">🔗</a></h2><p>Large, self‑supervised, general‑purpose models adaptable to many downstream tasks.</p>
<ul>
<li><strong>Examples</strong> : GPT‑4 (text), CLIP (image+text), DALL·E (image), SAM (vision segmentation).</li>
<li><strong>Benefits</strong> : reuse, performance, economy of scale.</li>
<li><strong>Risks</strong> : bias, compute cost, ecological footprint, opacity.</li>
</ul>
<hr>
<h2 id="8-scaling-laws--emergent-abilities">8 · Scaling Laws &amp; Emergent Abilities <a href="#8-scaling-laws--emergent-abilities" class="anchor">🔗</a></h2><ul>
<li>Empirical power‑law links between loss ↔ parameters, data, compute (OpenAI, DeepMind).</li>
<li><strong>Emergence</strong> : qualitative jumps (few‑shot learning, tool use) above certain scale (≈10 B+, 100 B+ params).</li>
<li><strong>Implications</strong> : unpredictable behaviours, but strong generalization—drives interest in alignment &amp; evals.</li>
</ul>
<hr>
<h2 id="9-variationalautoencodervae--quickrecap">9 · Variational Autoencoder (VAE) — Quick Recap <a href="#9-variationalautoencodervae--quickrecap" class="anchor">🔗</a></h2><ol>
<li><strong>Encoder</strong> → μ, σ² (latent distribution).</li>
<li><strong>Reparameterization trick</strong> → sample <em>z</em>.</li>
<li><strong>Decoder</strong> → reconstruct/generate <em>x̂</em>.</li>
<li><strong>Loss</strong> = Reconstruction Loss + KL‑Divergence( q(z|x) ‖ N(0,1) ).</li>
</ol>
<table>
<thead>
<tr>
<th>Strength</th>
<th>Why It Matters</th>
</tr>
</thead>
<tbody>
<tr>
<td>Generative</td>
<td>New images/text variants</td>
</tr>
<tr>
<td>Smooth latent space</td>
<td>Interpolation, arithmetic</td>
</tr>
<tr>
<td>Structured</td>
<td>Semi‑supervised &amp; controllable</td>
</tr>
<tr>
<td>Stable training</td>
<td>No adversarial collapse (vs GAN)</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="10-quick-cheatsheet--which-technique-when">10 · Quick Cheat‑Sheet — Which Technique When? <a href="#10-quick-cheatsheet--which-technique-when" class="anchor">🔗</a></h2><ul>
<li><strong>Need a fast tweak?</strong> → Prompt Engineering.</li>
<li><strong>Domain‑specific answers?</strong> → SFT or LoRA.</li>
<li><strong>Politeness / helpfulness?</strong> → RLHF.</li>
<li><strong>Up‑to‑date factuality?</strong> → RAG.</li>
<li><strong>Creative image/text synthesis?</strong> → Diffusion, VAE, GAN.</li>
</ul>
<hr>

    </div>

    
    
    

</section>


    </main>
    
    <footer id="footer">
    
        <div id="social">


    <a class="symbol" href="mailto:shashankforworkshekhar@gmail.com" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28" viewBox="0 0 72 72" xmlns="http://www.w3.org/2000/svg">
    <rect x="0" y="0" width="72" height="72" rx="8" ry="8" fill="#bbbbbb" />
    <polyline points="12,22 36,40 60,22" fill="none" stroke="#FFFFFF" stroke-width="3"/>
    <rect x="12" y="22" width="48" height="28" fill="none" stroke="#FFFFFF" stroke-width="3" />
    <line x1="12" y1="50" x2="60" y2="50" stroke="#FFFFFF" stroke-width="3"/>
</svg>

    </a>

    <a class="symbol" href="https://github.com/thatShashankGuy/" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28"  viewBox="0 0 72 72" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    
    <title>Github</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Social-Icons---Rounded-Black" transform="translate(-264.000000, -939.000000)">
            <g id="Github" transform="translate(264.000000, 939.000000)">
                <path d="M8,72 L64,72 C68.418278,72 72,68.418278 72,64 L72,8 C72,3.581722 68.418278,-8.11624501e-16 64,0 L8,0 C3.581722,8.11624501e-16 -5.41083001e-16,3.581722 0,8 L0,64 C5.41083001e-16,68.418278 3.581722,72 8,72 Z" id="Rounded" fill="#bbbbbb"></path>
                <path d="M35.9985,13 C22.746,13 12,23.7870921 12,37.096644 C12,47.7406712 18.876,56.7718301 28.4145,59.9584121 C29.6145,60.1797862 30.0525,59.4358488 30.0525,58.7973276 C30.0525,58.2250681 30.0315,56.7100863 30.0195,54.6996482 C23.343,56.1558981 21.9345,51.4693938 21.9345,51.4693938 C20.844,48.6864054 19.2705,47.9454799 19.2705,47.9454799 C17.091,46.4500754 19.4355,46.4801943 19.4355,46.4801943 C21.843,46.6503662 23.1105,48.9634994 23.1105,48.9634994 C25.2525,52.6455377 28.728,51.5823398 30.096,50.9649018 C30.3135,49.4077535 30.9345,48.3460615 31.62,47.7436831 C26.2905,47.1352808 20.688,45.0691228 20.688,35.8361671 C20.688,33.2052792 21.6225,31.0547881 23.1585,29.3696344 C22.911,28.7597262 22.0875,26.3110578 23.3925,22.9934585 C23.3925,22.9934585 25.4085,22.3459017 29.9925,25.4632101 C31.908,24.9285993 33.96,24.6620468 36.0015,24.6515052 C38.04,24.6620468 40.0935,24.9285993 42.0105,25.4632101 C46.5915,22.3459017 48.603,22.9934585 48.603,22.9934585 C49.9125,26.3110578 49.089,28.7597262 48.8415,29.3696344 C50.3805,31.0547881 51.309,33.2052792 51.309,35.8361671 C51.309,45.0917119 45.6975,47.1292571 40.3515,47.7256117 C41.2125,48.4695491 41.9805,49.9393525 41.9805,52.1877301 C41.9805,55.4089489 41.9505,58.0067059 41.9505,58.7973276 C41.9505,59.4418726 42.3825,60.1918338 43.6005,59.9554002 C53.13,56.7627944 60,47.7376593 60,37.096644 C60,23.7870921 49.254,13 35.9985,13" fill="#FFFFFF"></path>
            </g>
        </g>
    </g>
</svg>
    </a>

    <a class="symbol" href="https://www.linkedin.com/in/thatshashanguy/" rel="me" target="_blank">
        
        <svg width="28" height="28" fill="#bbbbbb" viewBox="0 0 500 500">
  <g fill="none" fill-rule="evenodd">
    <rect width="500" height="500" fill="#bbbbbb" rx="50"/>
    <path fill="#FFF" d="M154.703 100.183c-19.121 0-34.689 15.565-34.703 34.701 0 19.136 15.568 34.704 34.703 34.704 19.128 0 34.688-15.568 34.688-34.704 0-19.134-15.561-34.701-34.688-34.701zm26.045 83.348h-52.094a4.488 4.488 0 0 0-4.488 4.489v167.675a4.488 4.488 0 0 0 4.488 4.488h52.093a4.49 4.49 0 0 0 4.489-4.488V188.02a4.486 4.486 0 0 0-4.488-4.489zm133.176-1.974c-19.064 0-35.817 5.805-46.04 15.271v-8.808c0-2.48-2.01-4.489-4.489-4.489h-49.971a4.489 4.489 0 0 0-4.489 4.489v167.675a4.488 4.488 0 0 0 4.489 4.488h52.044a4.49 4.49 0 0 0 4.489-4.488v-82.957c0-23.802 4.378-38.555 26.227-38.555 21.526.026 23.137 15.846 23.137 39.977v81.535a4.489 4.489 0 0 0 4.49 4.488h52.068a4.489 4.489 0 0 0 4.488-4.488v-91.977c-.001-38.253-7.553-82.161-66.443-82.161z"/>
  </g>
</svg>

    </a>


</div>

    

    <div class="copyright">
    
       © Copyright 
       2025 
       <span class="split">
        <svg fill="#bbbbbb" width="15" height="15" version="1.1" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15px" height="15px" viewBox="0 0 15 15">
  <path d="M13.91,6.75c-1.17,2.25-4.3,5.31-6.07,6.94c-0.1903,0.1718-0.4797,0.1718-0.67,0C5.39,12.06,2.26,9,1.09,6.75&#xA;&#x9;C-1.48,1.8,5-1.5,7.5,3.45C10-1.5,16.48,1.8,13.91,6.75z"/>
</svg>
       </span>
       Shashank Shekhar
    
    </div>

    
      <div class="powerby">
        Powered by <a href='http://www.gohugo.io/'>Hugo</a> 
      </div>
    
</footer>



  </body>
</html>
