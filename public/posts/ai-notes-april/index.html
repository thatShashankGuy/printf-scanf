<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Brief Notes on GenAI from April | PrintF/ScanF</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="1Â Â· Machineâ€“Learning Model Taxonomy ğŸ”—1.1Â DiscriminativeÂ vsÂ Generative ğŸ”— Discriminative models learn the conditional distribution $P(y\mid x)$ to separate classes.
Primary useÂ Â· classification/regression. Typical natureÂ Â· often deterministic at inference. ExamplesÂ Â· LogisticÂ Regression, SVM, plain Feedâ€‘forward NNs, ResNetâ€‘like CNNs. Generative models learn the joint distribution $P(x,y)=P(x\mid y)P(y)$ and can synthesize newÂ x.
Primary useÂ Â· both classification and data generation/simulation. Typical natureÂ Â· probabilistic/stochastic. ExamplesÂ Â· NaÃ¯veÂ Bayes, HiddenÂ MarkovÂ Models, GANs, VAEs, Diffusion models.">
<meta name="generator" content="Hugo 0.126.1">


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">



<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />








  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">â†</span>Home</a>
	
	<a href="/about">About</a>
	<a href="/externals">References</a>
	<a href="/posts">Archive</a>
	<a href="/tags" style="display: none;">Tags</a>	
	

	

	
	
	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Brief Notes on GenAI from April</h1>

    <div class="tip">
        <time datetime="2025-05-02 00:00:00 &#43;0000 UTC">May 2, 2025</time>
        <span class="split">
          Â·
        </span>
        <span>
          811 words
        </span>
        <span class="split">
          Â·
        </span>
        <span>
          4 minute read
        </span>
    </div>

    
    


    <div class="content">
      <p><p class="markdown-image">
  <img src="/notesonai.png" alt="headline image"  title="Note taking with AI" />
</p></p>
<h2 id="1-machinelearning-model-taxonomy">1Â Â· Machineâ€“Learning Model Taxonomy <a href="#1-machinelearning-model-taxonomy" class="anchor">ğŸ”—</a></h2><h3 id="11discriminativevsgenerative">1.1Â DiscriminativeÂ vsÂ Generative <a href="#11discriminativevsgenerative" class="anchor">ğŸ”—</a></h3><ul>
<li>
<p><strong>Discriminative models</strong> learn the conditional distribution $P(y\mid x)$ to <strong>separate</strong> classes.</p>
<ul>
<li><em>Primary use</em>Â Â· classification/regression.</li>
<li><em>Typical nature</em>Â Â· often deterministic at inference.</li>
<li><em>Examples</em>Â Â· LogisticÂ Regression, SVM, plain Feedâ€‘forward NNs, ResNetâ€‘like CNNs.</li>
</ul>
</li>
<li>
<p><strong>Generative models</strong> learn the joint distribution $P(x,y)=P(x\mid y)P(y)$ and can <strong>synthesize newÂ x</strong>.</p>
<ul>
<li><em>Primary use</em>Â Â· both classification <strong>and</strong> data generation/simulation.</li>
<li><em>Typical nature</em>Â Â· probabilistic/stochastic.</li>
<li><em>Examples</em>Â Â· NaÃ¯veÂ Bayes, HiddenÂ MarkovÂ Models, GANs, VAEs, Diffusion models.</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Discriminative</th>
<th>Generative</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learns</td>
<td>$P(y\mid x)$</td>
<td>$P(x,y)$ or $P(x\mid y)P(y)$</td>
</tr>
<tr>
<td>Produces</td>
<td>Class label / score</td>
<td>Sample, likelihood <strong>or</strong> label</td>
</tr>
<tr>
<td>Strength</td>
<td>Decision boundaries</td>
<td>Full data distribution</td>
</tr>
<tr>
<td>Weakness</td>
<td>Needs labelled data</td>
<td>Harder to train/scale</td>
</tr>
</tbody>
</table>
<h3 id="12deterministicvsprobabilistic">1.2Â DeterministicÂ vsÂ Probabilistic <a href="#12deterministicvsprobabilistic" class="anchor">ğŸ”—</a></h3><table>
<thead>
<tr>
<th>Aspect</th>
<th>Deterministic</th>
<th>Probabilistic</th>
</tr>
</thead>
<tbody>
<tr>
<td>Output</td>
<td>Same value for same input</td>
<td>Distribution or random sample</td>
</tr>
<tr>
<td>Pros</td>
<td>Fast, predictable</td>
<td>Express uncertainty, robust to missing data</td>
</tr>
<tr>
<td>Cons</td>
<td>Ignores uncertainty</td>
<td>More compute, stochastic results</td>
</tr>
<tr>
<td>Examples</td>
<td>Decisionâ€‘Tree inference, SVM, frozen CNN</td>
<td>NaÃ¯veÂ Bayes, GaussianÂ Mixture, BayesianÂ Network, VAEÂ decoder</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="2-vectors-modalities">2Â Â· Vectors &amp;Â Modalities <a href="#2-vectors-modalities" class="anchor">ğŸ”—</a></h2><h3 id="21vectorsembeddings">2.1Â VectorsÂ (Embeddings) <a href="#21vectorsembeddings" class="anchor">ğŸ”—</a></h3><p><em>Definition</em>Â Â· Numeric representations that encode the semantics of entities (words, images, usersÂ â€¦).</p>
<ul>
<li>Enable similarity search (<code>cosine</code>, <code>dot</code>), clustering, recommendation, retrievalâ€‘augmented generation.</li>
<li>Learned via Word2Vec, GloVe, fastText, BERT, CLIP, etc.</li>
</ul>
<h3 id="22modalvsmultimodal">2.2Â ModalÂ vsÂ Multimodal <a href="#22modalvsmultimodal" class="anchor">ğŸ”—</a></h3><ul>
<li>
<p><strong>Modal</strong>Â = one input type (text <em>or</em> image <em>or</em> audio).</p>
</li>
<li>
<p><strong>Multimodal</strong>Â = multiple types jointly (e.g., imageÂ +Â caption).</p>
<ul>
<li>Rising trend: visionâ€“language models (e.g., GPTâ€‘4V, Gemini), audioâ€‘text (e.g., Whisper), videoâ€‘text.</li>
</ul>
</li>
</ul>
<h3 id="23neural-languagemodels">2.3Â Neural LanguageÂ Models <a href="#23neural-languagemodels" class="anchor">ğŸ”—</a></h3><p>Large neural nets pretrained on text with <strong>selfâ€‘supervised</strong> objectives.</p>
<ul>
<li>Families: <strong>GPTâ€‘n</strong> (decoderâ€‘only), <strong>BERT</strong> (encoderâ€‘only), <strong>T5Â /Â BART</strong> (encoderâ€‘decoder), <strong>LLaMA</strong> series.</li>
<li>Tasks: text generation, summarization, translation, code completion, search ranking.</li>
</ul>
<hr>
<h2 id="3-transformer-family">3Â Â· Transformer Family <a href="#3-transformer-family" class="anchor">ğŸ”—</a></h2><h3 id="31core-architecture-attentionisallyouneed2017">3.1Â Core Architecture Â (â€œAttentionÂ isÂ AllÂ YouÂ Needâ€,Â 2017) <a href="#31core-architecture-attentionisallyouneed2017" class="anchor">ğŸ”—</a></h3><ol>
<li>
<p><strong>Input embeddingsÂ + positional encodings</strong>.</p>
</li>
<li>
<p>Repeated <em>N</em>Â Ã— blocks:</p>
<ul>
<li>Multiâ€‘head <em>selfâ€‘attention</em> Â â†’ AddÂ &amp;Â LayerNorm.</li>
<li>Positionâ€‘wise Feedâ€‘ForwardÂ NN Â â†’ AddÂ &amp;Â LayerNorm.</li>
</ul>
</li>
<li>
<p>(Decoder adds masked selfâ€‘attention + crossâ€‘attention to encoder outputs.)</p>
</li>
</ol>
<h3 id="32selfattentionquickintuition">3.2Â Selfâ€‘AttentionÂ (QuickÂ Intuition) <a href="#32selfattentionquickintuition" class="anchor">ğŸ”—</a></h3><p>Each token forms a <strong>Query (Q)</strong> vector that is matched against <strong>Keys (K)</strong> of every token; the similarities weight the <strong>Values (V)</strong> to build a contextâ€‘aware representation.</p>
<h3 id="33transformervariants">3.3Â TransformerÂ Variants <a href="#33transformervariants" class="anchor">ğŸ”—</a></h3><table>
<thead>
<tr>
<th>Variant</th>
<th>Architecture</th>
<th>Flagship Models</th>
<th>Typical Tasks</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Encoderâ€‘only</strong></td>
<td>Autoâ€‘encoding</td>
<td>BERT, RoBERTa</td>
<td>Classification, QA, embeddings</td>
</tr>
<tr>
<td><strong>Decoderâ€‘only</strong></td>
<td>Autoâ€‘regressive</td>
<td>GPT, LLaMAâ€‘2Â Chat</td>
<td>Text/code generation</td>
</tr>
<tr>
<td><strong>Encoderâ€‘Decoder</strong></td>
<td>Seqâ€‘toâ€‘Seq</td>
<td>T5, BART, Pegasus</td>
<td>Translation, summarization</td>
</tr>
</tbody>
</table>
<h3 id="34languagemodeling-objectives">3.4Â Languageâ€‘Modeling Objectives <a href="#34languagemodeling-objectives" class="anchor">ğŸ”—</a></h3><table>
<thead>
<tr>
<th>Objective</th>
<th>Context Used</th>
<th>Predicts</th>
<th>Archetype</th>
</tr>
</thead>
<tbody>
<tr>
<td>MaskedÂ (MLM)</td>
<td>Bidirectional</td>
<td>Masked tokens</td>
<td>BERT</td>
</tr>
<tr>
<td>AutoregressiveÂ (AR)</td>
<td>Leftâ€‘toâ€‘right (or rightâ€‘toâ€‘left)</td>
<td>Next token</td>
<td>GPT</td>
</tr>
<tr>
<td>PrefixÂ LM</td>
<td>PastÂ tokens (unmasked) + full prefix</td>
<td>Next token</td>
<td>T5 preâ€‘training</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="4-learning-paradigms">4Â Â· Learning Paradigms <a href="#4-learning-paradigms" class="anchor">ğŸ”—</a></h2><table>
<thead>
<tr>
<th>Paradigm</th>
<th>Labels</th>
<th>Model Learns</th>
<th>Canonical Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Supervised</strong></td>
<td>Explicit</td>
<td>Map <em>xÂ â†’Â y</em></td>
<td>ImageNet classification</td>
</tr>
<tr>
<td><strong>Unsupervised</strong></td>
<td>None</td>
<td>Structure of <em>x</em></td>
<td>Word2Vec, PCA, clustering</td>
</tr>
<tr>
<td><strong>Selfâ€‘Supervised</strong></td>
<td>Labels from data</td>
<td>Predict masked / future parts</td>
<td>GPT preâ€‘training, BERT MLM</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="5-model-adaptation-finetuning">5Â Â· Model Adaptation &amp;Â Fineâ€‘Tuning <a href="#5-model-adaptation-finetuning" class="anchor">ğŸ”—</a></h2><table>
<thead>
<tr>
<th>Technique</th>
<th>Data Need</th>
<th>Compute</th>
<th>Brief</th>
</tr>
</thead>
<tbody>
<tr>
<td>PromptÂ Engineering</td>
<td>None (zeroâ€‘shot)</td>
<td>â€“</td>
<td>Steer behavior via instructions/examples</td>
</tr>
<tr>
<td>SupervisedÂ Fineâ€‘TuningÂ (SFT)</td>
<td>Labelled pairs</td>
<td>High</td>
<td>Adjust all weights to taskÂ domain</td>
</tr>
<tr>
<td><strong>LoRAÂ /Â Adapters</strong></td>
<td>Labelled pairs</td>
<td>Lowâ€“Med</td>
<td>Train tiny rankâ€‘update layers; mergeable</td>
</tr>
<tr>
<td>RLHF</td>
<td>Human preference scores</td>
<td>VeryÂ High</td>
<td>Align model to helpful/safe outputs</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="6-prompt-engineering">6Â Â· Prompt Engineering <a href="#6-prompt-engineering" class="anchor">ğŸ”—</a></h2><h3 id="61anatomy-of-a-good-prompt">6.1Â Anatomy of a Good Prompt <a href="#61anatomy-of-a-good-prompt" class="anchor">ğŸ”—</a></h3><p><strong>Instruction â†’ Context â†’ Input â†’ Outputâ€‘format â†’ Tone/Role</strong></p>
<h3 id="62prompting-techniques">6.2Â Prompting Techniques <a href="#62prompting-techniques" class="anchor">ğŸ”—</a></h3><table>
<thead>
<tr>
<th>Technique</th>
<th>BestÂ For</th>
<th>Key Idea</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zeroâ€‘Shot</td>
<td>Simple, common tasks</td>
<td>Ask directly</td>
</tr>
<tr>
<td>Fewâ€‘Shot</td>
<td>Pattern imitation</td>
<td>Give 2â€“5 exemplars</td>
</tr>
<tr>
<td>Chainâ€‘ofâ€‘Thought</td>
<td>Reasoning/maths</td>
<td>â€œLetâ€™s think stepÂ byÂ stepâ€</td>
</tr>
<tr>
<td>Selfâ€‘Consistency</td>
<td>Reliable CoT answer</td>
<td>SampleÂ K reasoning paths, majority vote</td>
</tr>
<tr>
<td>ReAct</td>
<td>Toolâ€‘using agents</td>
<td>Interleave reasoning &amp; external actions</td>
</tr>
<tr>
<td>Treeâ€‘ofâ€‘Thought</td>
<td>Complex planning</td>
<td>Explore multiple branches, backtrack</td>
</tr>
<tr>
<td>Retrievalâ€‘AugmentedÂ (RAG)</td>
<td>Factual or domain answers</td>
<td>Retrieve docs â†’ feed as context</td>
</tr>
</tbody>
</table>
<h3 id="63security-concerns-in-prompting">6.3Â Security Concerns in Prompting <a href="#63security-concerns-in-prompting" class="anchor">ğŸ”—</a></h3><ol>
<li><strong>Prompt injection / jailbreaks</strong></li>
<li><strong>Data leakage</strong> (keys, PII)</li>
<li><strong>Prompt leakage</strong> (system prompt exposure)</li>
<li><strong>Malicious content generation</strong> (spam, phishing, code exploits)</li>
<li><strong>Token flooding / promptÂ DOS</strong></li>
</ol>
<p><em>Mitigations</em>Â Â· input sanitation, guardrail LLMs, content filters, maxâ€‘token limits, rateâ€‘limits, redâ€‘teaming.</p>
<hr>
<h2 id="7-foundation-models">7Â Â· Foundation Models <a href="#7-foundation-models" class="anchor">ğŸ”—</a></h2><p>Large, selfâ€‘supervised, generalâ€‘purpose models adaptable to many downstream tasks.</p>
<ul>
<li><strong>Examples</strong>Â : GPTâ€‘4Â (text), CLIPÂ (image+text), DALLÂ·EÂ (image), SAMÂ (vision segmentation).</li>
<li><strong>Benefits</strong>Â : reuse, performance, economy of scale.</li>
<li><strong>Risks</strong>Â : bias, compute cost, ecological footprint, opacity.</li>
</ul>
<hr>
<h2 id="8-scaling-laws--emergent-abilities">8Â Â· Scaling Laws &amp; Emergent Abilities <a href="#8-scaling-laws--emergent-abilities" class="anchor">ğŸ”—</a></h2><ul>
<li>Empirical powerâ€‘law links between loss â†” parameters, data, compute (OpenAI, DeepMind).</li>
<li><strong>Emergence</strong>Â : qualitative jumps (fewâ€‘shot learning, tool use) above certain scale (â‰ˆ10â€¯B+, 100â€¯B+ params).</li>
<li><strong>Implications</strong>Â : unpredictable behaviours, but strong generalizationâ€”drives interest in alignment &amp; evals.</li>
</ul>
<hr>
<h2 id="9-variationalautoencodervae--quickrecap">9Â Â· VariationalÂ AutoencoderÂ (VAE) â€” QuickÂ Recap <a href="#9-variationalautoencodervae--quickrecap" class="anchor">ğŸ”—</a></h2><ol>
<li><strong>Encoder</strong> â†’Â Î¼,Â ÏƒÂ² (latent distribution).</li>
<li><strong>Reparameterization trick</strong> â†’ sampleÂ <em>z</em>.</li>
<li><strong>Decoder</strong> â†’ reconstruct/generate <em>xÌ‚</em>.</li>
<li><strong>Loss</strong> = ReconstructionÂ Loss + KLâ€‘Divergence(Â q(z|x)Â â€–Â N(0,1)Â ).</li>
</ol>
<table>
<thead>
<tr>
<th>Strength</th>
<th>Why It Matters</th>
</tr>
</thead>
<tbody>
<tr>
<td>Generative</td>
<td>New images/text variants</td>
</tr>
<tr>
<td>Smooth latent space</td>
<td>Interpolation, arithmetic</td>
</tr>
<tr>
<td>Structured</td>
<td>Semiâ€‘supervised &amp; controllable</td>
</tr>
<tr>
<td>Stable training</td>
<td>No adversarial collapse (vsÂ GAN)</td>
</tr>
</tbody>
</table>
<hr>
<h2 id="10-quick-cheatsheet--which-technique-when">10Â Â· Quick Cheatâ€‘Sheet â€” Which Technique When? <a href="#10-quick-cheatsheet--which-technique-when" class="anchor">ğŸ”—</a></h2><ul>
<li><strong>Need a fast tweak?</strong> â†’ PromptÂ Engineering.</li>
<li><strong>Domainâ€‘specific answers?</strong> â†’ SFT or LoRA.</li>
<li><strong>Politeness / helpfulness?</strong> â†’ RLHF.</li>
<li><strong>Upâ€‘toâ€‘date factuality?</strong> â†’ RAG.</li>
<li><strong>Creative image/text synthesis?</strong> â†’ Diffusion, VAE, GAN.</li>
</ul>
<hr>

    </div>

    
    
    

</section>


    </main>
    
    <footer id="footer">
    
        <div id="social">


    <a class="symbol" href="mailto:shashankforworkshekhar@gmail.com" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28" viewBox="0 0 72 72" xmlns="http://www.w3.org/2000/svg">
    <rect x="0" y="0" width="72" height="72" rx="8" ry="8" fill="#bbbbbb" />
    <polyline points="12,22 36,40 60,22" fill="none" stroke="#FFFFFF" stroke-width="3"/>
    <rect x="12" y="22" width="48" height="28" fill="none" stroke="#FFFFFF" stroke-width="3" />
    <line x1="12" y1="50" x2="60" y2="50" stroke="#FFFFFF" stroke-width="3"/>
</svg>

    </a>

    <a class="symbol" href="https://github.com/thatShashankGuy/" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28"  viewBox="0 0 72 72" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    
    <title>Github</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Social-Icons---Rounded-Black" transform="translate(-264.000000, -939.000000)">
            <g id="Github" transform="translate(264.000000, 939.000000)">
                <path d="M8,72 L64,72 C68.418278,72 72,68.418278 72,64 L72,8 C72,3.581722 68.418278,-8.11624501e-16 64,0 L8,0 C3.581722,8.11624501e-16 -5.41083001e-16,3.581722 0,8 L0,64 C5.41083001e-16,68.418278 3.581722,72 8,72 Z" id="Rounded" fill="#bbbbbb"></path>
                <path d="M35.9985,13 C22.746,13 12,23.7870921 12,37.096644 C12,47.7406712 18.876,56.7718301 28.4145,59.9584121 C29.6145,60.1797862 30.0525,59.4358488 30.0525,58.7973276 C30.0525,58.2250681 30.0315,56.7100863 30.0195,54.6996482 C23.343,56.1558981 21.9345,51.4693938 21.9345,51.4693938 C20.844,48.6864054 19.2705,47.9454799 19.2705,47.9454799 C17.091,46.4500754 19.4355,46.4801943 19.4355,46.4801943 C21.843,46.6503662 23.1105,48.9634994 23.1105,48.9634994 C25.2525,52.6455377 28.728,51.5823398 30.096,50.9649018 C30.3135,49.4077535 30.9345,48.3460615 31.62,47.7436831 C26.2905,47.1352808 20.688,45.0691228 20.688,35.8361671 C20.688,33.2052792 21.6225,31.0547881 23.1585,29.3696344 C22.911,28.7597262 22.0875,26.3110578 23.3925,22.9934585 C23.3925,22.9934585 25.4085,22.3459017 29.9925,25.4632101 C31.908,24.9285993 33.96,24.6620468 36.0015,24.6515052 C38.04,24.6620468 40.0935,24.9285993 42.0105,25.4632101 C46.5915,22.3459017 48.603,22.9934585 48.603,22.9934585 C49.9125,26.3110578 49.089,28.7597262 48.8415,29.3696344 C50.3805,31.0547881 51.309,33.2052792 51.309,35.8361671 C51.309,45.0917119 45.6975,47.1292571 40.3515,47.7256117 C41.2125,48.4695491 41.9805,49.9393525 41.9805,52.1877301 C41.9805,55.4089489 41.9505,58.0067059 41.9505,58.7973276 C41.9505,59.4418726 42.3825,60.1918338 43.6005,59.9554002 C53.13,56.7627944 60,47.7376593 60,37.096644 C60,23.7870921 49.254,13 35.9985,13" fill="#FFFFFF"></path>
            </g>
        </g>
    </g>
</svg>
    </a>

    <a class="symbol" href="https://www.linkedin.com/in/thatshashanguy/" rel="me" target="_blank">
        
        <svg width="28" height="28" fill="#bbbbbb" viewBox="0 0 500 500">
  <g fill="none" fill-rule="evenodd">
    <rect width="500" height="500" fill="#bbbbbb" rx="50"/>
    <path fill="#FFF" d="M154.703 100.183c-19.121 0-34.689 15.565-34.703 34.701 0 19.136 15.568 34.704 34.703 34.704 19.128 0 34.688-15.568 34.688-34.704 0-19.134-15.561-34.701-34.688-34.701zm26.045 83.348h-52.094a4.488 4.488 0 0 0-4.488 4.489v167.675a4.488 4.488 0 0 0 4.488 4.488h52.093a4.49 4.49 0 0 0 4.489-4.488V188.02a4.486 4.486 0 0 0-4.488-4.489zm133.176-1.974c-19.064 0-35.817 5.805-46.04 15.271v-8.808c0-2.48-2.01-4.489-4.489-4.489h-49.971a4.489 4.489 0 0 0-4.489 4.489v167.675a4.488 4.488 0 0 0 4.489 4.488h52.044a4.49 4.49 0 0 0 4.489-4.488v-82.957c0-23.802 4.378-38.555 26.227-38.555 21.526.026 23.137 15.846 23.137 39.977v81.535a4.489 4.489 0 0 0 4.49 4.488h52.068a4.489 4.489 0 0 0 4.488-4.488v-91.977c-.001-38.253-7.553-82.161-66.443-82.161z"/>
  </g>
</svg>

    </a>


</div>

    

    <div class="copyright">
    
       Â© Copyright 
       2025 
       <span class="split">
        <svg fill="#bbbbbb" width="15" height="15" version="1.1" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15px" height="15px" viewBox="0 0 15 15">
  <path d="M13.91,6.75c-1.17,2.25-4.3,5.31-6.07,6.94c-0.1903,0.1718-0.4797,0.1718-0.67,0C5.39,12.06,2.26,9,1.09,6.75&#xA;&#x9;C-1.48,1.8,5-1.5,7.5,3.45C10-1.5,16.48,1.8,13.91,6.75z"/>
</svg>
       </span>
       Shashank Shekhar
    
    </div>

    
      <div class="powerby">
        Powered by <a href='http://www.gohugo.io/'>Hugo</a> 
      </div>
    
</footer>



  </body>
</html>
